<!-- Copyright 2022 The MediaPipe Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. -->
<script
		src="./jquery-3.6.1.slim.min.js"
		integrity="sha256-w8CvhFs7iHNVUtnSP0YKEg00p9Ih13rlL9zGqvLdePA="
		crossorigin="anonymous"></script>

<link href="./assets/gesturesStyles.css" rel="stylesheet">
<link href="./material-components-web.min.css" rel="stylesheet">

<script src="./material-components-web.min.js"></script>
<script src="./drawing_utils.js" crossorigin="anonymous"></script>
<script src="./hands.js" crossorigin="anonymous"></script>
<script src="./scripts/squiggles.js"></script>
<!-- <script src="./tasks-vision.js" crossorigin="anonymous"></script> -->

<!-- <h1>Recognize hand gestures using the MediaPipe HandGestureRecognizer task</h1> -->
<div id="display">
  <!-- Canvas Elements Will be populated here. -->
</div>

<section id="demos" class="invisible" style="display: none">
<!-- <section id="demos" class="invisible"> -->
  <h2>Demo: Recognize gestures</h2>
  <p><em>Click on an image below</em> to identify the gestures in the image.</p>

  <div class="detectOnClick">
    <img src="https://assets.codepen.io/9177687/idea-gcbe74dc69_1920.jpg" crossorigin="anonymous" title="Click to get recognize!" />
    <p class="classification removed">
  </div>
  <div class="detectOnClick">
    <img src="https://assets.codepen.io/9177687/thumbs-up-ga409ddbd6_1.png" crossorigin="anonymous" title="Click to get recognize!" />
    <p class="classification removed">
  </div>

  <h2><br>Demo: Webcam continuous hand gesture detection</h2>
  <p>Use your hand to make gestures in front of the camera to get gesture classification. </br>Click <b>enable webcam</b> below and grant access to the webcam if prompted.</p>

  <div id="liveView" class="videoView">
    <button id="webcamButton" class="mdc-button mdc-button--raised">
      <span class="mdc-button__ripple"></span>
      <span class="mdc-button__label">ENABLE WEBCAM</span>
    </button>
    <div style="position: relative;">
      <video id="webcam" autoplay playsinline></video>
      <canvas class="output_canvas" id="output_canvas" width="1280" height="720" style="position: absolute; left: 0px; top: 0px;"></canvas>
      <p id='gesture_output' class="output">
    </div>
  </div>
</section>

<script type="module">
  // Copyright 2022 The MediaPipe Authors.

// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at

//      http://www.apache.org/licenses/LICENSE-2.0

// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
// Copyright 2022 The MediaPipe Authors.
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//      http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
import vision from "https://cdn.skypack.dev/@mediapipe/tasks-vision@latest";
const { GestureRecognizer, FilesetResolver } = vision;
const demosSection = document.getElementById("demos");
let gestureRecognizer;
let runningMode = "VIDEO";
let enableWebcamButton;
let webcamRunning = true;
const videoHeight = "360px";
const videoWidth = "480px";
//const webcamRunning = true;
// Before we can use HandLandmarker class we must wait for it to finish
// loading. Machine Learning models can be large and take a moment to
// get everything needed to run.
async function runDemo() {
    const vision = await FilesetResolver.forVisionTasks("./wasm");
    gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
        baseOptions: {
            modelAssetPath: "./gesture_recognizer.task"
        },
        runningMode: runningMode
    });
    demosSection.classList.remove("invisible");
    const constraints = {
      video: true
    };
    navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {
      video.srcObject = stream;
      video.addEventListener("loadeddata", predictWebcam);
      //window.requestAnimationFrame(predictWebcam);
    });
    webcamRunning = true;
    //enableCam()
}
runDemo();
/********************************************************************
// Demo 1: Grab a bunch of images from the page and detection them
// upon click.
********************************************************************/
// In this demo, we have put all our clickable images in divs with the
// CSS class 'detectionOnClick'. Lets get all the elements that have
// this class.
const imageContainers = document.getElementsByClassName("detectOnClick");
// Now let's go through all of these and add a click event listener.
//for (let i = 0; i < imageContainers.length; i++) {
    // Add event listener to the child element whichis the img element.
  //  imageContainers[i].children[0].addEventListener("click", handleClick);
//}
// When an image is clicked, let's detect it and display results!

/********************************************************************
// Demo 2: Continuously grab image from webcam stream and detect it.
********************************************************************/
const video = document.getElementById("webcam");
const canvasElement = document.getElementById("output_canvas");
const canvasCtx = canvasElement.getContext("2d");
const gestureOutput = document.getElementById("gesture_output");
// Check if webcam access is supported.
function hasGetUserMedia() {
    return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
}
// If webcam supported, add event listener to button for when user
// wants to activate it.

// Enable the live webcam view and start detection.
async function predictWebcam() {
    const webcamElement = document.getElementById("webcam");
    // Now let's start detecting the stream.
    // await gestureRecognizer.setOptions({ runningMode: runningMode });
    //if (runningMode === "IMAGE") {
    //    runningMode = "VIDEO";
    //    await gestureRecognizer.setOptions({ runningMode: runningMode });
    //}
    let nowInMs = Date.now();
    const results = await gestureRecognizer.recognizeForVideo(video, nowInMs);
    canvasCtx.save();
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
    canvasElement.style.height = videoHeight;
    webcamElement.style.height = videoHeight;
    canvasElement.style.width = videoWidth;
    webcamElement.style.width = videoWidth;
    //console.log(results);
    if (results.landmarks) {
        for (const landmarks of results.landmarks) {

            // Goes directly to pointer finger... Have to choose one.
            let pointerFinger = landmarks[8]
            let viewportWidth = parseFloat(window.innerWidth);
            let viewportHeight = parseFloat(window.innerHeight);

            let x = parseFloat(pointerFinger.x.toPrecision(8));
            let y = parseFloat(pointerFinger.y.toPrecision(8));

            // x: Math.floor((1 - x) * viewportWidth),
            window.handPoint = {
              x: Math.floor((x) * viewportWidth),
              y: Math.floor((y) * viewportHeight)
            };
            //drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {
                //color: "#00FF00",
                //lineWidth: 5
            //});
            //drawLandmarks(canvasCtx, landmarks, { color: "#FF0000", lineWidth: 2 });
        }
    }
    canvasCtx.restore();
    if (results.gestures.length > 0) {
        //g_color = 
        console.log("Found a gesture");
        console.log(results.gestures[0][0].categoryName);
        gestureOutput.style.display = "block";
        gestureOutput.style.width = videoWidth;
        
        gestureOutput.innerText =
            "GestureRecognizer: " +
                results.gestures[0][0].categoryName +
                "\n Confidence: " +
                Math.round(parseFloat(results.gestures[0][0].score) * 100) +
                "%";
        
        window.g_color = "#" + Math.floor(Math.random()*16777215).toString(16);
    }
    else {
        gestureOutput.style.display = "none";
    }
    // Call this function again to keep predicting when the browser is ready.
    if (webcamRunning === true) {
        window.requestAnimationFrame(predictWebcam);
    }
}


</script>

<style>